{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "# %%pycodestyle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from pandasql import sqldf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "\n",
    "\n",
    "# Lambdas\n",
    "\n",
    "def q(x):\n",
    "    return sqldf(x, globals())\n",
    "\n",
    "# Randomize given the value options\n",
    "\n",
    "\n",
    "def randomize_value_choice(value_options):\n",
    "    \"\"\"\n",
    "    The value_options format is either:\n",
    "        [True, False] - True or False\n",
    "        [1,2,3,4,5] - 1 to 5\n",
    "        [[0,1,4],5] - Any subset of [0,1,2,3,4] and 5\n",
    "            represents the number of features in the dataframe\n",
    "    \"\"\"\n",
    "    value = None\n",
    "    # Check to make sure there are value options\n",
    "    if len(value_options) > 0:\n",
    "        # Make sure it isn't a 2D List\n",
    "        if type(value_options[0]) != list:\n",
    "            # Simple choice from the list -\n",
    "                # In the future this might include a distribution mean+std\n",
    "            np.random.choice(value_options)\n",
    "        else:\n",
    "            # If the option is a 2D array it means we take\n",
    "                # each element with 50% probability\n",
    "            value = []\n",
    "            for element in value_options[0]:\n",
    "                random_number = np.random.rand(1)[0]\n",
    "                if random_number > 0.5:\n",
    "                    value.append(element)\n",
    "    return value\n",
    "\n",
    "# My own version of RandomizedSearchCV\n",
    "\n",
    "\n",
    "def RandomizedGridSearchCV(n_experiments, pipe,\n",
    "                           param_distributions, train_X, train_y,\n",
    "                           test_X, test_y,\n",
    "                           scoring='neg_mean_squared_error', cv=2):\n",
    "\n",
    "    # Transform the param_distributions into four arrays\n",
    "    key_list = []\n",
    "    transform_class_list = []\n",
    "    parameter_name_list = []\n",
    "    value_options_list = []\n",
    "    for key, value_options in param_distributions.items():\n",
    "        class_key, parameter_name = key.split(\"__\")\n",
    "        transform_class = pipe.named_steps[class_key]\n",
    "        key_list.append(key)\n",
    "        transform_class_list.append(transform_class)\n",
    "        parameter_name_list.append(parameter_name)\n",
    "        value_options_list.append(value_options)\n",
    "\n",
    "    # Initialize experiments dictionary\n",
    "    experiments = {}\n",
    "    for key, transform_class, parameter_name, value_options in \\\n",
    "        zip(key_list, transform_class_list, parameter_name_list,\n",
    "            value_options_list):\n",
    "        if type(value_options[0]) != list:\n",
    "            experiments[key] = []\n",
    "        else:\n",
    "            # value_options[1] is the number of features\n",
    "            for i in range(value_options[1]):\n",
    "                experiments[key + \"_\" + str(i)] = []\n",
    "    experiments['score'] = []\n",
    "\n",
    "    # Iterate over the experiments\n",
    "    for iteration in range(n_experiments):\n",
    "        print(\"Iteration: \", iteration)\n",
    "\n",
    "        # Updates the transform parameters\n",
    "        for key, transform_class, parameter_name, value_options in \\\n",
    "            zip(key_list, transform_class_list, parameter_name_list,\n",
    "                value_options_list):\n",
    "\n",
    "            # Get the random value\n",
    "            value = randomize_value_choice(value_options)\n",
    "\n",
    "            # Set the value\n",
    "            setattr(transform_class, parameter_name, value)\n",
    "\n",
    "            # If the value option is a 2D array it is treated differently\n",
    "            if type(value_options[0]) != list:\n",
    "                experiments[key].append(value)\n",
    "            else:\n",
    "                for i in range(value_options[1]):\n",
    "                    experiments[key + \"_\" + str(i)].append(value)\n",
    "                    \n",
    "        # Fit\n",
    "        pipe.fit(train_X, train_y)\n",
    "        \n",
    "        # Predict\n",
    "        pred_y = pipe.predict(test_X)\n",
    "        \n",
    "        # Scoring\n",
    "        if scoring == 'neg_mean_squared_error':\n",
    "            score = mean_squared_error(pred_y, test_y)\n",
    "        else:\n",
    "            raise Exception('Scoring type not implemented')\n",
    "        \n",
    "        # Appending the score\n",
    "        experiments[\"score\"].append(score)\n",
    "\n",
    "    experiments_df = pd.DataFrame(experiments)\n",
    "    return experiments_df\n",
    "\n",
    "# Standard scaler data preparation class\n",
    "\n",
    "\n",
    "class StandardScalerTransform(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_indices_to_replace=[]):\n",
    "        print(np.c_[X])        \n",
    "        self.column_indices_to_replace = column_indices_to_replace\n",
    "        self.standard_scalers = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for index in self.column_indices_to_replace:\n",
    "            self.standard_scalers[index] = StandardScaler()\n",
    "            self.standard_scalers[index].fit(X[:, index])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for index in self.column_indices_to_replace:\n",
    "            X[:, index] = self.standard_scalers[index].transform(X[:, index])        \n",
    "        return np.c_[X]\n",
    "\n",
    "# Min-max scaler data preparation class\n",
    "\n",
    "\n",
    "class MinMaxScalerTransform(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_indices_to_replace=[]):\n",
    "        self.column_indices_to_replace = column_indices_to_replace\n",
    "        self.min_max_scalers = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for index in self.column_indices_to_replace:\n",
    "            self.min_max_scalers[index] = MinMaxScaler()\n",
    "            self.min_max_scalers[index].fit(X[:, index])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for index in self.column_indices_to_replace:\n",
    "            X[:, index] = self.min_max_scalers[index].transform(X[:, index])\n",
    "        return np.c_[X]\n",
    "\n",
    "# Generic xgboost fit using several grid searches\n",
    "\n",
    "\n",
    "def get_xgboost_model(train_X, train_y):\n",
    "    model = Pipeline([('xgb', XGBRegressor())])\n",
    "\n",
    "    # 1) Tune max depth\n",
    "    param_grid = [{\n",
    "        'xgb__n_estimators': [100],\n",
    "        'xgb__learning_rate': [0.1],\n",
    "        'xgb__max_depth': [1, 2, 4, 6, 8],\n",
    "        'xgb__subsample': [1.00]\n",
    "    }]\n",
    "    gs1 = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=2)\n",
    "    gs1 = gs1.fit(train_X, train_y)\n",
    "    max_depth = gs1.best_params_['xgb__max_depth']\n",
    "    # print(gs1.best_score_)\n",
    "    # print(gs1.best_params_)\n",
    "\n",
    "    # 2) Tune subsample\n",
    "    param_grid = [{\n",
    "        'xgb__n_estimators': [100],\n",
    "        'xgb__learning_rate': [0.1],\n",
    "        'xgb__max_depth': [max_depth],\n",
    "        'xgb__subsample': [0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 1.00]\n",
    "    }]\n",
    "    gs2 = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=2)\n",
    "    gs2 = gs2.fit(train_X, train_y)\n",
    "    subsample = gs2.best_params_['xgb__subsample']\n",
    "    # print(gs2.best_score_)\n",
    "    # print(gs2.best_params_)\n",
    "\n",
    "    # 3) Tune n_estimators\n",
    "    param_grid = [{\n",
    "        'xgb__n_estimators': [50, 100, 150, 200],\n",
    "        'xgb__learning_rate': [0.1],\n",
    "        'xgb__max_depth': [max_depth],\n",
    "        'xgb__subsample': [subsample]\n",
    "    }]\n",
    "    gs3 = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=2)\n",
    "    gs3 = gs3.fit(train_X, train_y)\n",
    "    n_estimators = gs3.best_params_['xgb__n_estimators']\n",
    "    # print(gs3.best_score_)\n",
    "    # print(gs3.best_params_)\n",
    "\n",
    "    # 4) Tune learning rate\n",
    "    param_grid = [{\n",
    "        'xgb__n_estimators': [n_estimators],\n",
    "        'xgb__learning_rate': [0.1],\n",
    "        'xgb__max_depth': [max_depth],\n",
    "        'xgb__subsample': [subsample]\n",
    "    },\n",
    "        {\n",
    "        'xgb__n_estimators': [n_estimators * 3],\n",
    "        'xgb__learning_rate': [0.03],\n",
    "        'xgb__max_depth': [max_depth],\n",
    "        'xgb__subsample': [subsample]\n",
    "    }]\n",
    "    gs4 = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=2)\n",
    "    gs4 = gs4.fit(train_X, train_y)\n",
    "    n_estimators = gs4.best_params_['xgb__n_estimators']\n",
    "    learning_rate = gs4.best_params_['xgb__learning_rate']\n",
    "    # print(gs4.best_score_)\n",
    "    # print(gs4.best_params_)\n",
    "\n",
    "    # 5) Tune n_estimators\n",
    "    param_grid = [{\n",
    "        'xgb__n_estimators': [\n",
    "            int(0.8 * n_estimators),\n",
    "            int(1.0 * n_estimators),\n",
    "            int(1.2 * n_estimators)\n",
    "        ],\n",
    "        'xgb__learning_rate': [learning_rate],\n",
    "        'xgb__max_depth': [max_depth],\n",
    "        'xgb__subsample': [subsample]\n",
    "    }]\n",
    "    gs5 = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=2)\n",
    "    gs5 = gs5.fit(train_X, train_y)\n",
    "    n_estimators = gs5.best_params_['xgb__n_estimators']\n",
    "    # print(gs5.best_score_)\n",
    "    # print(gs5.best_params_)\n",
    "\n",
    "    # 6) Tune sampling by tree\n",
    "    param_grid = [{\n",
    "        'xgb__n_estimators': [n_estimators],\n",
    "        'xgb__learning_rate': [learning_rate],\n",
    "        'xgb__max_depth': [max_depth],\n",
    "        'xgb__subsample': [subsample],\n",
    "        'xgb__colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'xgb__colsample_bylevel': [1.0]\n",
    "    }]\n",
    "    gs6 = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=2)\n",
    "    gs6 = gs6.fit(train_X, train_y)\n",
    "    colsample_bytree = gs6.best_params_['xgb__colsample_bytree']\n",
    "    # print(gs6.best_score_)\n",
    "    # print(gs6.best_params_)\n",
    "\n",
    "    # 7) Tune subsample\n",
    "    param_grid = [{\n",
    "        'xgb__n_estimators': [n_estimators],\n",
    "        'xgb__learning_rate': [learning_rate],\n",
    "        'xgb__max_depth': [max_depth],\n",
    "        'xgb__subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'xgb__colsample_bytree': [colsample_bytree],\n",
    "        'xgb__colsample_bylevel': [1.0]\n",
    "    }]\n",
    "    gs7 = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=2)\n",
    "    gs7 = gs7.fit(train_X, train_y)\n",
    "    subsample = gs7.best_params_['xgb__subsample']\n",
    "    # print(gs7.best_score_)\n",
    "    # print(gs7.best_params_)\n",
    "\n",
    "    # 8) Tune sampling by level\n",
    "    n_estimators = gs7.best_params_['xgb__n_estimators']\n",
    "    param_grid = [{\n",
    "        'xgb__n_estimators': [n_estimators],\n",
    "        'xgb__learning_rate': [learning_rate],\n",
    "        'xgb__max_depth': [max_depth],\n",
    "        'xgb__subsample': [subsample],\n",
    "        'xgb__colsample_bytree': [colsample_bytree],\n",
    "        'xgb__colsample_bylevel': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    }]\n",
    "    gs8 = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=2)\n",
    "    gs8 = gs8.fit(train_X, train_y)\n",
    "    colsample_bylevel = gs8.best_params_['xgb__colsample_bylevel']\n",
    "    # print(gs8.best_score_)\n",
    "    # print(gs8.best_params_)\n",
    "\n",
    "    # 9) Tune sampling fields\n",
    "    n_estimators = gs8.best_params_['xgb__n_estimators']\n",
    "    subsample = 0.9 if subsample == 1.0 else subsample\n",
    "    colsample_bytree = 0.6 if colsample_bytree == 0.5 else colsample_bytree\n",
    "    colsample_bylevel = 0.9 if colsample_bylevel == 1.0 else colsample_bylevel\n",
    "    param_grid = [{\n",
    "        'xgb__n_estimators': [n_estimators],\n",
    "        'xgb__learning_rate': [learning_rate],\n",
    "        'xgb__max_depth': [max_depth],\n",
    "        'xgb__subsample': [subsample, subsample + 0.1],\n",
    "        'xgb__colsample_bytree': [colsample_bytree - 0.1, colsample_bytree],\n",
    "        'xgb__colsample_bylevel': [colsample_bylevel, colsample_bylevel + 0.1]\n",
    "    }]\n",
    "    gs9 = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=2)\n",
    "    gs9 = gs9.fit(train_X, train_y)\n",
    "    subsample = gs9.best_params_['xgb__subsample']\n",
    "    colsample_bytree = gs9.best_params_['xgb__colsample_bytree']\n",
    "    colsample_bylevel = gs9.best_params_['xgb__colsample_bylevel']\n",
    "    # print(gs9.best_score_)\n",
    "    # print(gs9.best_params_)\n",
    "\n",
    "    # 10) Tune alpha\n",
    "    param_grid = [{\n",
    "        'xgb__n_estimators': [n_estimators],\n",
    "        'xgb__learning_rate': [learning_rate],\n",
    "        'xgb__max_depth': [max_depth],\n",
    "        'xgb__subsample': [subsample],\n",
    "        'xgb__colsample_bytree': [colsample_bytree],\n",
    "        'xgb__colsample_bylevel': [colsample_bylevel],\n",
    "        'xgb__reg_lambda': [0.001, 0.01, 0.1, 0.3, 1, 3, 10, 100, 1000]\n",
    "    }]\n",
    "    gs10 = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=2)\n",
    "    gs10 = gs10.fit(train_X, train_y)\n",
    "    # print(gs10.best_score_)\n",
    "    # print(gs10.best_params_)\n",
    "\n",
    "    # Find the best model\n",
    "    # Sometimes the best model isn't the last one, so checking all of them\n",
    "    best_model = gs1\n",
    "    best_model_score = gs1.best_score_\n",
    "    if gs2.best_score_ > best_model_score:\n",
    "        best_model = gs2\n",
    "        best_model_score = gs2.best_score_\n",
    "    if gs2.best_score_ > best_model_score:\n",
    "        best_model = gs2\n",
    "        best_model_score = gs2.best_score_\n",
    "    if gs3.best_score_ > best_model_score:\n",
    "        best_model = gs3\n",
    "        best_model_score = gs3.best_score_\n",
    "    if gs4.best_score_ > best_model_score:\n",
    "        best_model = gs4\n",
    "        best_model_score = gs4.best_score_\n",
    "    if gs5.best_score_ > best_model_score:\n",
    "        best_model = gs5\n",
    "        best_model_score = gs5.best_score_\n",
    "    if gs6.best_score_ > best_model_score:\n",
    "        best_model = gs6\n",
    "        best_model_score = gs6.best_score_\n",
    "    if gs7.best_score_ > best_model_score:\n",
    "        best_model = gs7\n",
    "        best_model_score = gs7.best_score_\n",
    "    if gs8.best_score_ > best_model_score:\n",
    "        best_model = gs8\n",
    "        best_model_score = gs8.best_score_\n",
    "    if gs9.best_score_ > best_model_score:\n",
    "        best_model = gs9\n",
    "        best_model_score = gs9.best_score_\n",
    "    if gs10.best_score_ > best_model_score:\n",
    "        best_model = gs10\n",
    "        best_model_score = gs10.best_score_\n",
    "\n",
    "    # Return the best model\n",
    "    return XGBRegressor(**best_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n",
      "Iteration:  0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1.50234e+01 6.27390e-01 3.46600e-02 7.05042e+00 7.25800e-01 1.91860e-01\n 3.96100e-02 2.05500e-02 1.51772e+01 1.44383e+01 3.73800e-02 6.88800e-02\n 4.12380e-01 1.39134e+01 6.58800e-02 8.40540e-01 1.73310e-01 8.24400e-02\n 2.06080e-01 1.40300e-01 7.35341e+01 1.50980e-01 1.41500e-01 3.51140e-01\n 1.87000e-02 9.10300e-02 3.53501e+00 3.57800e-02 3.87350e-01 6.72400e-02\n 1.35472e+00 2.22120e-01 2.33099e+00 6.44405e+00 3.30600e-02 1.43200e-02\n 1.43900e-02 7.50260e-01 7.84200e-01 6.46600e-02 4.37900e-02 3.75780e-01\n 4.15292e+01 4.29400e-02 1.41385e+00 9.72418e+00 9.88430e-01 5.26930e-01\n 5.58107e+00 9.92485e+00 2.98500e-02 1.31580e-01 1.71420e-01 1.05393e+00\n 1.55757e+01 4.54192e+00 3.23700e-02 6.79208e+01 6.04700e-02 1.49320e-01\n 1.07930e-01 1.81590e-01 7.61620e-01 1.00245e+00 5.20140e-01 1.02330e+01\n 6.71910e-01 1.44550e-01 1.11320e-01 1.28020e-01 8.01400e-02 1.22358e+00\n 3.56868e+00 1.30580e-01 1.42310e-01 6.66400e-02 8.66400e-02 1.14600e-01\n 2.77974e+00 1.11081e+01 7.99248e+00 8.98296e+00 6.12700e-02 3.58090e-01\n 6.71772e+00 1.62864e+00 5.66998e+00 5.78900e-02 3.83684e+00 2.30040e+00\n 1.77830e-01 1.33598e+01 2.50461e+01 2.18700e-02 1.90730e-01 2.63630e-01\n 1.10874e+01 2.37934e+00 4.20300e-02 1.12658e+00 6.23560e-01 5.51500e-02\n 3.55100e-02 1.64390e-01 2.92400e+00 1.51902e+00 3.15000e-02 4.62960e-01\n 7.89600e-02 7.90410e-01 4.75237e+00 3.68940e-01 1.44760e-01 9.06000e-03\n 9.26600e-02 2.81838e+00 3.84970e+00 2.48017e+01 2.98190e-01 5.34120e-01\n 5.11830e-01 2.43938e+01 4.87141e+00 9.74400e-02 4.01100e-02 5.44520e-01\n 4.89822e+00 1.96570e-01 3.87100e-02 2.36482e+01 1.03280e-01 1.00840e-01\n 5.30200e-02 7.85700e-01 8.82900e-02 3.47428e+00 6.07600e-02 1.30100e-02\n 1.34284e+00 1.65660e+00 5.42500e-02 7.67202e+00 8.30800e-02 4.02020e-01\n 2.24890e-01 2.00849e+01 2.11610e-01 4.46200e-02 1.75050e-01 2.45220e-01\n 1.80028e+00 6.39312e+00 5.56100e-02 5.37200e-02 3.76800e-02 9.82349e+00\n 2.15505e+00 5.87205e+00 2.36862e+00 7.36711e+00 4.29700e-02 1.50380e-01\n 2.07460e-01 1.15040e-01 4.09740e+00 9.25200e-02 9.60400e-02 1.20830e-01\n 1.70900e-02 9.29900e-02 1.00080e-01 2.17700e-02 3.39830e-01 2.37857e+00\n 3.53700e-02 4.30100e-02 5.11358e+01 9.91655e+00 1.96500e-02 1.69020e-01\n 5.47900e-02 6.14700e-01 1.20482e+01 1.14250e-01 8.81250e-01 8.79212e+00\n 7.88600e-02 5.02300e-02 8.89762e+01 5.82401e+00 5.20177e+00 1.41030e-01\n 8.19900e-02 6.53876e+00 1.36781e+01 1.23290e-01 5.78000e-02 2.63548e+00\n 2.49800e-02 5.08300e-02 4.83567e+00 8.20058e+00 3.31470e-01 3.69200e-01\n 2.24236e+00 3.22640e-01 4.66600e-02 6.63510e-01 5.75290e-01 1.71340e-01\n 6.89900e-02 7.24400e-02 3.15330e-01 2.07162e+01 6.15100e-02 2.59150e-01\n 1.09600e-02 1.80846e+01 1.31170e-01 1.84982e+01 7.52601e+00 3.29820e-01\n 1.35222e+01 1.22690e-01 1.78990e-01 3.58400e-02 1.50100e-02 5.73500e-02\n 1.02900e-01 5.60200e-02 1.58603e+01 1.42502e+00 9.37800e-02 6.41700e-02\n 7.72990e-01 1.20742e+00 3.32105e+00 9.59571e+00 2.89900e-02 4.07710e-01\n 1.22040e-01 4.33700e-02 1.13290e-01 1.52880e+01 9.18702e+00 6.64200e-02\n 1.27440e-01 2.20511e+01 5.29305e+00 2.29690e-01 6.12900e-02 4.81900e-02\n 1.08342e+01 6.90500e-02 1.53800e-02 8.24809e+00 1.48660e-01 3.82140e-01\n 1.00623e+01 1.40520e-01 1.22472e+01 2.31390e+00 8.18700e-02 3.61500e-02\n 1.98020e-01 1.71710e-01 2.29270e-01 1.38799e+00 5.78340e-01 2.41030e-01\n 1.77800e-02 5.44114e+00 9.55770e-01 8.64476e+00 5.37000e-01 5.40110e-01\n 4.59000e-02 1.83377e+00 9.33889e+00 2.49800e-01 1.10270e-01 5.57780e-01\n 3.25430e-01 5.73116e+00 2.11240e-01 3.03470e-01 1.30751e+01 1.95100e-02\n 4.41700e-02 6.37960e-01 2.44668e+00 3.35900e-02 1.78667e+01 3.16360e+00\n 1.19511e+01 4.56000e-02 2.10380e-01 9.39063e+00 1.09590e-01 3.04100e-02\n 5.20580e-01 2.51990e-01 2.17190e-01 1.29320e-01 6.65492e+00 2.14090e-01\n 2.79570e-01 7.83932e+00 1.00000e-01 6.21100e-02 9.06500e-02 3.44500e-02\n 1.46336e+00 1.59360e-01 7.01300e-02 1.42362e+01 9.06800e-02 3.49400e-01\n 6.56650e-01 1.32620e-01 4.98100e-02 8.15174e+00 2.73100e-02 6.28807e+00\n 1.50860e-01 2.19770e-01 1.18123e+01 4.11300e-02 1.36420e-01 1.61282e+00\n 8.49213e+00 8.25260e-01 3.76619e+01 3.69695e+00 3.93200e-02 5.49700e-02\n 1.43337e+01 5.36000e-02 3.11300e-02 5.50070e-01 1.06120e-01 6.29760e-01\n 2.53560e-01 5.66000e-02 2.25971e+01 2.21880e-01 2.01019e+00 6.61700e-02\n 2.39120e-01 9.76170e-01 7.50300e-02 5.69175e+00 4.75470e-01 1.27570e-01\n 1.36000e-02 4.22239e+00 8.87300e-02 3.69311e+00 8.44700e-02 1.06718e+01\n 8.37000e-02 4.52700e-02 5.82115e+00 7.87500e-02 2.44953e+00 1.54450e-01\n 2.53870e-01 3.04900e-02 3.30450e-01 8.22100e-02 8.52040e-01 2.69380e-01\n 6.80117e+00 1.27346e+00 1.04690e-01 9.96654e+00 6.91100e-02 1.68118e+01\n 8.26500e-02 2.86558e+01 2.54300e-02 6.11540e-01 4.92980e-01 2.73397e+00\n 3.40060e-01 1.49632e+00 4.26131e+00 6.86000e-02 8.26725e+00 7.15100e-02\n 7.75223e+00 4.54400e-02 2.89550e-01 3.77498e+00 7.16500e-02 4.74100e-02\n 1.25179e+00 1.25790e-01 1.58760e-01 1.71200e-01 2.99160e-01 1.50100e-02\n 1.11604e+01 2.28760e-01].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-2db0701d4511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtest_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     cv=2)\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Sort the scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-573a86e44b51>\u001b[0m in \u001b[0;36mRandomizedGridSearchCV\u001b[0;34m(n_experiments, pipe, param_distributions, train_X, train_y, test_X, test_y, scoring, cv)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Code/commons/venv/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Code/commons/venv/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    229\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Code/commons/venv/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Code/commons/venv/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, **fit_params)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Code/commons/venv/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-573a86e44b51>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_indices_to_replace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_scalers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_scalers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Code/commons/venv/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Code/commons/venv/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    647\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m    648\u001b[0m                         \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Code/commons/venv/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    550\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1.50234e+01 6.27390e-01 3.46600e-02 7.05042e+00 7.25800e-01 1.91860e-01\n 3.96100e-02 2.05500e-02 1.51772e+01 1.44383e+01 3.73800e-02 6.88800e-02\n 4.12380e-01 1.39134e+01 6.58800e-02 8.40540e-01 1.73310e-01 8.24400e-02\n 2.06080e-01 1.40300e-01 7.35341e+01 1.50980e-01 1.41500e-01 3.51140e-01\n 1.87000e-02 9.10300e-02 3.53501e+00 3.57800e-02 3.87350e-01 6.72400e-02\n 1.35472e+00 2.22120e-01 2.33099e+00 6.44405e+00 3.30600e-02 1.43200e-02\n 1.43900e-02 7.50260e-01 7.84200e-01 6.46600e-02 4.37900e-02 3.75780e-01\n 4.15292e+01 4.29400e-02 1.41385e+00 9.72418e+00 9.88430e-01 5.26930e-01\n 5.58107e+00 9.92485e+00 2.98500e-02 1.31580e-01 1.71420e-01 1.05393e+00\n 1.55757e+01 4.54192e+00 3.23700e-02 6.79208e+01 6.04700e-02 1.49320e-01\n 1.07930e-01 1.81590e-01 7.61620e-01 1.00245e+00 5.20140e-01 1.02330e+01\n 6.71910e-01 1.44550e-01 1.11320e-01 1.28020e-01 8.01400e-02 1.22358e+00\n 3.56868e+00 1.30580e-01 1.42310e-01 6.66400e-02 8.66400e-02 1.14600e-01\n 2.77974e+00 1.11081e+01 7.99248e+00 8.98296e+00 6.12700e-02 3.58090e-01\n 6.71772e+00 1.62864e+00 5.66998e+00 5.78900e-02 3.83684e+00 2.30040e+00\n 1.77830e-01 1.33598e+01 2.50461e+01 2.18700e-02 1.90730e-01 2.63630e-01\n 1.10874e+01 2.37934e+00 4.20300e-02 1.12658e+00 6.23560e-01 5.51500e-02\n 3.55100e-02 1.64390e-01 2.92400e+00 1.51902e+00 3.15000e-02 4.62960e-01\n 7.89600e-02 7.90410e-01 4.75237e+00 3.68940e-01 1.44760e-01 9.06000e-03\n 9.26600e-02 2.81838e+00 3.84970e+00 2.48017e+01 2.98190e-01 5.34120e-01\n 5.11830e-01 2.43938e+01 4.87141e+00 9.74400e-02 4.01100e-02 5.44520e-01\n 4.89822e+00 1.96570e-01 3.87100e-02 2.36482e+01 1.03280e-01 1.00840e-01\n 5.30200e-02 7.85700e-01 8.82900e-02 3.47428e+00 6.07600e-02 1.30100e-02\n 1.34284e+00 1.65660e+00 5.42500e-02 7.67202e+00 8.30800e-02 4.02020e-01\n 2.24890e-01 2.00849e+01 2.11610e-01 4.46200e-02 1.75050e-01 2.45220e-01\n 1.80028e+00 6.39312e+00 5.56100e-02 5.37200e-02 3.76800e-02 9.82349e+00\n 2.15505e+00 5.87205e+00 2.36862e+00 7.36711e+00 4.29700e-02 1.50380e-01\n 2.07460e-01 1.15040e-01 4.09740e+00 9.25200e-02 9.60400e-02 1.20830e-01\n 1.70900e-02 9.29900e-02 1.00080e-01 2.17700e-02 3.39830e-01 2.37857e+00\n 3.53700e-02 4.30100e-02 5.11358e+01 9.91655e+00 1.96500e-02 1.69020e-01\n 5.47900e-02 6.14700e-01 1.20482e+01 1.14250e-01 8.81250e-01 8.79212e+00\n 7.88600e-02 5.02300e-02 8.89762e+01 5.82401e+00 5.20177e+00 1.41030e-01\n 8.19900e-02 6.53876e+00 1.36781e+01 1.23290e-01 5.78000e-02 2.63548e+00\n 2.49800e-02 5.08300e-02 4.83567e+00 8.20058e+00 3.31470e-01 3.69200e-01\n 2.24236e+00 3.22640e-01 4.66600e-02 6.63510e-01 5.75290e-01 1.71340e-01\n 6.89900e-02 7.24400e-02 3.15330e-01 2.07162e+01 6.15100e-02 2.59150e-01\n 1.09600e-02 1.80846e+01 1.31170e-01 1.84982e+01 7.52601e+00 3.29820e-01\n 1.35222e+01 1.22690e-01 1.78990e-01 3.58400e-02 1.50100e-02 5.73500e-02\n 1.02900e-01 5.60200e-02 1.58603e+01 1.42502e+00 9.37800e-02 6.41700e-02\n 7.72990e-01 1.20742e+00 3.32105e+00 9.59571e+00 2.89900e-02 4.07710e-01\n 1.22040e-01 4.33700e-02 1.13290e-01 1.52880e+01 9.18702e+00 6.64200e-02\n 1.27440e-01 2.20511e+01 5.29305e+00 2.29690e-01 6.12900e-02 4.81900e-02\n 1.08342e+01 6.90500e-02 1.53800e-02 8.24809e+00 1.48660e-01 3.82140e-01\n 1.00623e+01 1.40520e-01 1.22472e+01 2.31390e+00 8.18700e-02 3.61500e-02\n 1.98020e-01 1.71710e-01 2.29270e-01 1.38799e+00 5.78340e-01 2.41030e-01\n 1.77800e-02 5.44114e+00 9.55770e-01 8.64476e+00 5.37000e-01 5.40110e-01\n 4.59000e-02 1.83377e+00 9.33889e+00 2.49800e-01 1.10270e-01 5.57780e-01\n 3.25430e-01 5.73116e+00 2.11240e-01 3.03470e-01 1.30751e+01 1.95100e-02\n 4.41700e-02 6.37960e-01 2.44668e+00 3.35900e-02 1.78667e+01 3.16360e+00\n 1.19511e+01 4.56000e-02 2.10380e-01 9.39063e+00 1.09590e-01 3.04100e-02\n 5.20580e-01 2.51990e-01 2.17190e-01 1.29320e-01 6.65492e+00 2.14090e-01\n 2.79570e-01 7.83932e+00 1.00000e-01 6.21100e-02 9.06500e-02 3.44500e-02\n 1.46336e+00 1.59360e-01 7.01300e-02 1.42362e+01 9.06800e-02 3.49400e-01\n 6.56650e-01 1.32620e-01 4.98100e-02 8.15174e+00 2.73100e-02 6.28807e+00\n 1.50860e-01 2.19770e-01 1.18123e+01 4.11300e-02 1.36420e-01 1.61282e+00\n 8.49213e+00 8.25260e-01 3.76619e+01 3.69695e+00 3.93200e-02 5.49700e-02\n 1.43337e+01 5.36000e-02 3.11300e-02 5.50070e-01 1.06120e-01 6.29760e-01\n 2.53560e-01 5.66000e-02 2.25971e+01 2.21880e-01 2.01019e+00 6.61700e-02\n 2.39120e-01 9.76170e-01 7.50300e-02 5.69175e+00 4.75470e-01 1.27570e-01\n 1.36000e-02 4.22239e+00 8.87300e-02 3.69311e+00 8.44700e-02 1.06718e+01\n 8.37000e-02 4.52700e-02 5.82115e+00 7.87500e-02 2.44953e+00 1.54450e-01\n 2.53870e-01 3.04900e-02 3.30450e-01 8.22100e-02 8.52040e-01 2.69380e-01\n 6.80117e+00 1.27346e+00 1.04690e-01 9.96654e+00 6.91100e-02 1.68118e+01\n 8.26500e-02 2.86558e+01 2.54300e-02 6.11540e-01 4.92980e-01 2.73397e+00\n 3.40060e-01 1.49632e+00 4.26131e+00 6.86000e-02 8.26725e+00 7.15100e-02\n 7.75223e+00 4.54400e-02 2.89550e-01 3.77498e+00 7.16500e-02 4.74100e-02\n 1.25179e+00 1.25790e-01 1.58760e-01 1.71200e-01 2.99160e-01 1.50100e-02\n 1.11604e+01 2.28760e-01].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Example dataset\n",
    "boston_data = load_boston()\n",
    "\n",
    "# Extract pandas dataframe and target\n",
    "X = pd.DataFrame(boston_data['data'])\n",
    "y = pd.DataFrame(boston_data['target'])\n",
    "\n",
    "# Train/test split\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "train_X, test_X = train_X.values, test_X.values\n",
    "train_y, test_y = train_y.values.reshape(-1, 1), test_y.values.reshape(-1, 1)\n",
    "\n",
    "# An okay model fit to the data\n",
    "# xgb_model = get_xgboost_model(train_X, train_y)\n",
    "\n",
    "# Pipeline\n",
    "pipe = Pipeline([('standard_scaler', StandardScalerTransform()),\n",
    "               #  ('min_max_scaler', MinMaxScalerTransform()),\n",
    "                 ('model', xgb_model)])\n",
    "\n",
    "# Find the number of features\n",
    "num_features = train_X.shape[1]\n",
    "\n",
    "# Testing with these indices\n",
    "indices = list(range(num_features))\n",
    "\n",
    "# Possible configurations\n",
    "param_distributions = {\n",
    "    'standard_scaler__column_indices_to_replace': [indices, num_features] # ,\n",
    "#    'min_max_scaler__column_indices_to_replace': [indices, num_features]\n",
    "}\n",
    "\n",
    "# Randomly search the space n_iter times\n",
    "experiments_df = RandomizedGridSearchCV(\n",
    "    n_experiments=10,\n",
    "    pipe=pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    train_X=train_X,\n",
    "    train_y=train_y,\n",
    "    test_X=test_X,\n",
    "    test_y=test_y,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=2)\n",
    "\n",
    "# Sort the scores\n",
    "experiments_df.sort_values(by=['score'], ascending=False, inplace=True)\n",
    "\n",
    "# Drop score\n",
    "experiments_X_df = experiments_df.drop(['score'], axis=1)\n",
    "\n",
    "# Get column names\n",
    "X_column_names = experiments_X_df.columns\n",
    "\n",
    "# Convert to numpy\n",
    "experiments_X = experiments_X_df.values\n",
    "experiments_y = experiments_df[['score']].values\n",
    "\n",
    "# Create an XGBoost model tuned with the experiments data\n",
    "xgb_experiments_model = get_xgboost_model(experiments_X, experiments_y)\n",
    "\n",
    "# Fit the model\n",
    "xgb_experiments_model.fit(experiments_X_df, experiments_y)\n",
    "\n",
    "# Extract shap values\n",
    "explainer = shap.TreeExplainer(xgb_experiments_model)\n",
    "shap_values = explainer.shap_values(experiments_X_df)\n",
    "\n",
    "# Shap as dataframe\n",
    "pandas_shap_df = pd.DataFrame(shap_values, columns=X_column_names)\n",
    "pandas_shap_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1, xgb__learning_rate=0.03, xgb__max_depth=1,\n",
       "       xgb__n_estimators=450, xgb__subsample=1.0)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_experiments_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.266728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.266728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.266728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.266728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.266728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.266728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.266728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.266728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.266728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.266728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score\n",
       "0  7.266728\n",
       "1  7.266728\n",
       "2  7.266728\n",
       "3  7.266728\n",
       "4  7.266728\n",
       "5  7.266728\n",
       "6  7.266728\n",
       "7  7.266728\n",
       "8  7.266728\n",
       "9  7.266728"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'polarized_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-68a180ce7938>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas_shap_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexperiments_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mpolarized_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpandas_shap_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mpolarized_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'polarized_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Transformation to polarized groups of shap values\n",
    "polarized_shap_df = pandas_shap_df.copy()\n",
    "for i in range(0, len(pandas_shap_df.index)):\n",
    "    for j in range(0, len(pandas_shap_df.columns)):\n",
    "        if not experiments_df.iloc[i, j]:\n",
    "            polarized_df.iloc[i, j] = -1 * pandas_shap_df.iloc[i, j]\n",
    "polarized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certainly, I set a feature to True for large positive values\n",
    "# Also, I set a feature to False for large negative values\n",
    "# Otherwise, it is set to True or False\n",
    "polarized_shap_result = polarized_df.sum()\n",
    "polarized_shap_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commons",
   "language": "python",
   "name": "commons"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
