{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "- Place %%pycodestyle at the top of any cell to check python syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm is installed...but failed to load!\n"
     ]
    }
   ],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "import shap\n",
    "\n",
    "# Import from my GitHub\n",
    "from getxgboostmodel.getxgboostmodel import get_xgboost_model\n",
    "from randomizedgridsearch.randomizedgridsearch import RandomizedGridSearch\n",
    "from transformers.transformers import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "boston_data = load_boston()\n",
    "\n",
    "# Extract pandas dataframe and target\n",
    "X = pd.DataFrame(boston_data['data']).copy().values\n",
    "y = pd.DataFrame(boston_data['target']).copy().values\n",
    "\n",
    "# Train/test split\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42)\n",
    "train_X, test_X = train_X, test_X\n",
    "train_y, test_y = train_y.reshape(-1, 1), test_y.reshape(-1, 1)\n",
    "\n",
    "# An okay model fit to the data\n",
    "try:\n",
    "    xgb_model\n",
    "except:\n",
    "    xgb_model = get_xgboost_model(train_X, train_y)\n",
    "\n",
    "# Pipeline\n",
    "pipe = Pipeline([('standard_scaler', StandardScalerTransform()),\n",
    "                 ('min_max_scaler', MinMaxScalerTransform()),\n",
    "                 ('binarizer', BinarizerTransform()), \n",
    "#                  ('k_bins_discretizer', KBinsDiscretizerTransform()),\n",
    "#                  ('k_bins_discretizer2', KBinsDiscretizerTransform2()),\n",
    "#                  ('k_bins_discretizer3', KBinsDiscretizerTransform3()),                 \n",
    "                 ('model', xgb_model)])\n",
    "\n",
    "# Find the number of features\n",
    "num_features = train_X.shape[1]\n",
    "\n",
    "# Testing with these indices\n",
    "indices = list(range(num_features))\n",
    "\n",
    "# Scalers\n",
    "scaler_values = [0.05]*num_features\n",
    "\n",
    "# Possible configurations [None, True, or False] - None means not decided yet\n",
    "param_distributions = {\n",
    "    'standard_scaler': scaler_values,\n",
    "    'min_max_scaler': scaler_values,\n",
    "    'binarizer': scaler_values,    \n",
    "}\n",
    "\n",
    "experiments_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 19.07it/s]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(20):\n",
    "    # Randomly search the space n_iter times\n",
    "    experiments_results_temp = RandomizedGridSearch(\n",
    "        n_experiments=100,\n",
    "        pipe=pipe,\n",
    "        param_distributions=param_distributions,\n",
    "        train_X=train_X,\n",
    "        train_y=train_y,\n",
    "        test_X=test_X,\n",
    "        test_y=test_y,\n",
    "        scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Append to experiment results\n",
    "    experiments_results = experiments_results.append(experiments_results_temp, ignore_index=True)\n",
    "    \n",
    "    # Ignore past data\n",
    "    # experiments_results = experiments_results_temp\n",
    "\n",
    "    # Drop score\n",
    "    experiments_X_df = experiments_results.drop(['score'], axis=1)\n",
    "\n",
    "    # Get column names\n",
    "    X_column_names = experiments_X_df.columns\n",
    "\n",
    "    # Convert to numpy\n",
    "    experiments_X = experiments_X_df.values\n",
    "    experiments_y = experiments_results[['score']].values\n",
    "\n",
    "    # Create an XGBoost model tuned with the experiments data\n",
    "    try:\n",
    "        xgb_experiments_model\n",
    "        # Tune hyperparameters every once in a while\n",
    "        # if iteration % 7 == 6:\n",
    "        #     xgb_experiments_model = get_xgboost_model(experiments_X, experiments_y)\n",
    "    except:\n",
    "        xgb_experiments_model = get_xgboost_model(experiments_X, experiments_y)\n",
    "\n",
    "    # Fit the model\n",
    "    xgb_experiments_model.fit(experiments_X_df, experiments_y)\n",
    "\n",
    "    # Extract shap values\n",
    "    explainer = shap.TreeExplainer(xgb_experiments_model)\n",
    "    shap_values = explainer.shap_values(experiments_X_df)\n",
    "\n",
    "    # Shap as dataframe\n",
    "    shap_values_of_experiments = pd.DataFrame(shap_values, columns=X_column_names)\n",
    "    shap_values_of_experiments['score'] = experiments_y\n",
    "\n",
    "    # Function to support analysis\n",
    "    def find_significance_from_experiments_results(importance_threshold=0.05, max_toggles_to_lock_per_series=5):\n",
    "        temp_df = shap_values_of_experiments.drop(['score'], axis=1).copy()\n",
    "        for i in range(0, len(shap_values_of_experiments.index)):\n",
    "            for j in range(0, len(shap_values_of_experiments.columns)):\n",
    "                if not experiments_results.iloc[i, j]:\n",
    "                    temp_df.iloc[i, j] = -1 * shap_values_of_experiments.iloc[i, j]\n",
    "        temp_df = temp_df.sum().sort_values()\n",
    "        options_to_set_to_false = temp_df[temp_df > 0]\n",
    "        options_to_set_to_true = temp_df[temp_df < 0]    \n",
    "        sum_value = (options_to_set_to_false.sum() + abs(options_to_set_to_true.sum()))\n",
    "        options_to_set_to_false = options_to_set_to_false / sum_value\n",
    "        options_to_set_to_true = abs(options_to_set_to_true) / sum_value\n",
    "        options_to_set_to_false = options_to_set_to_false[options_to_set_to_false > importance_threshold].sort_values(ascending=False)\n",
    "        options_to_set_to_true = options_to_set_to_true[options_to_set_to_true > importance_threshold]\n",
    "        return options_to_set_to_false[0:max_toggles_to_lock_per_series], options_to_set_to_true[0:max_toggles_to_lock_per_series]\n",
    "\n",
    "    # Call function\n",
    "    options_to_set_to_false, options_to_set_to_true = find_significance_from_experiments_results()\n",
    "\n",
    "    # Make the set to true DF\n",
    "    options_to_set_to_true_df = pd.DataFrame()\n",
    "    transformation, value = None, None\n",
    "    try:\n",
    "        transformation_and_value = options_to_set_to_true.keys()\n",
    "    except:\n",
    "        transformation, value = [], []\n",
    "    if len(transformation_and_value) > 0:\n",
    "        options_to_set_to_true_df[\"transformation\"] = [x.split(\"__\")[0] for x in transformation_and_value]\n",
    "        options_to_set_to_true_df[\"value\"] = [x.split(\"__\")[1] for x in transformation_and_value]\n",
    "    else:\n",
    "        options_to_set_to_true_df[\"transformation\"] = []\n",
    "        options_to_set_to_true_df[\"value\"] = []\n",
    "    options_to_set_to_true_df[\"significance\"] = options_to_set_to_true.values\n",
    "\n",
    "    # Make the false DF\n",
    "    options_to_set_to_false_df = pd.DataFrame()\n",
    "    transformation, value = None, None\n",
    "    try:\n",
    "        transformation_and_value = options_to_set_to_false.keys()\n",
    "    except:\n",
    "        transformation, value = [], []\n",
    "    if len(transformation_and_value) > 0:\n",
    "        options_to_set_to_false_df[\"transformation\"] = [x.split(\"__\")[0] for x in transformation_and_value]\n",
    "        options_to_set_to_false_df[\"value\"] = [x.split(\"__\")[1] for x in transformation_and_value]\n",
    "    else:\n",
    "        options_to_set_to_false_df[\"transformation\"] = []\n",
    "        options_to_set_to_false_df[\"value\"] = []\n",
    "    options_to_set_to_false_df[\"significance\"] = options_to_set_to_false.values\n",
    "\n",
    "    # Reset updates\n",
    "    updates = 0\n",
    "    \n",
    "    # Set to True\n",
    "    for index, row in options_to_set_to_true_df.iterrows():            \n",
    "        if param_distributions[row['transformation']][int(row['value'])] < 0.9:\n",
    "            updates += 1\n",
    "        param_distributions[row['transformation']][int(row['value'])] *= 2.0        \n",
    "        if param_distributions[row['transformation']][int(row['value'])] >= 0.9:\n",
    "            param_distributions[row['transformation']][int(row['value'])] = 0.9        \n",
    "        \n",
    "\n",
    "    for index, row in options_to_set_to_false_df.iterrows():\n",
    "        if param_distributions[row['transformation']][int(row['value'])] > 0.01:\n",
    "            updates += 1        \n",
    "        param_distributions[row['transformation']][int(row['value'])] *= 0.5\n",
    "        if param_distributions[row['transformation']][int(row['value'])] <= 0.01:\n",
    "            param_distributions[row['transformation']][int(row['value'])] = 0.01\n",
    "\n",
    "    # End early if there is nothing to tune\n",
    "    print(\"Updates:\", updates)\n",
    "    if updates == 0:\n",
    "        break\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'standard_scaler': [0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.003125,\n",
       "  0.003125,\n",
       "  0.05,\n",
       "  0.003125,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.8],\n",
       " 'min_max_scaler': [0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.003125,\n",
       "  0.003125,\n",
       "  0.05,\n",
       "  0.003125,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.8],\n",
       " 'binarizer': [0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.003125,\n",
       "  0.003125,\n",
       "  0.05,\n",
       "  0.003125,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.05,\n",
       "  0.8]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'options_to_set_to_false_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-723c1f4070da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptions_to_set_to_false_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'options_to_set_to_false_df' is not defined"
     ]
    }
   ],
   "source": [
    "options_to_set_to_false_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_to_set_to_false, options_to_set_to_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_results.sort_values(by='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commons",
   "language": "python",
   "name": "commons"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
